{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn his model and change the MSA layers to nn.transformerencoder, also, try and change any other layer that is in built in pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Properties from research paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoder: stack N = 6 (identica layers)   \n",
    "dmodel = 512   \n",
    "\n",
    "decoder: stack N = 6 (identica layers)\n",
    "\n",
    "h = 8  \n",
    "dk,dv = 64   \n",
    "\n",
    "hidden layer of ffn = 2048   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(512/8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = nn.Embedding(4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(4, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero =torch.randn(1,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = F.softmax(zero, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7955, 0.0705, 0.0674, 0.0073, 0.0593]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_2 = torch.zeros(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.]]), tensor([[0]]))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_2, mn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb2 = nn.Embedding(65, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = torch.randint(4, (4,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 2, 3, 3, 2, 2, 3, 1],\n",
       "        [3, 2, 3, 0, 3, 3, 1, 1],\n",
       "        [0, 0, 3, 1, 1, 1, 3, 3],\n",
       "        [0, 0, 2, 0, 2, 2, 0, 0]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "mn = torch.multinomial(probs, num_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 2.]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((zero_2, mn), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb3 = nn.Embedding(32, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0548, -0.9601,  0.1128,  0.4235],\n",
       "        [-1.1208, -0.3069,  0.3056,  0.9976],\n",
       "        [ 0.1564,  0.2386, -1.0579, -0.2684],\n",
       "        [-2.8524,  1.3360, -1.2760, -0.4714],\n",
       "        [-0.2928,  0.8955, -0.6687, -1.0177],\n",
       "        [ 0.4759, -0.0696,  0.4178,  0.6019],\n",
       "        [ 0.1742,  1.5600,  1.1127,  1.3638],\n",
       "        [-1.0526,  1.3567,  1.4657,  1.8995],\n",
       "        [ 0.0511,  0.5668, -1.0124, -0.4460],\n",
       "        [-0.5721,  0.4174, -0.1807,  0.1996],\n",
       "        [ 0.8471,  0.4727, -0.2182,  0.9853],\n",
       "        [ 1.5313, -2.3779, -0.2422,  1.1341],\n",
       "        [ 0.3830, -0.3081,  0.7641,  1.3492],\n",
       "        [ 0.3422,  0.4366, -1.5697, -0.3675],\n",
       "        [ 0.0697,  0.1447, -0.3595, -1.4552],\n",
       "        [-0.9424, -0.8859,  0.5325,  0.9046],\n",
       "        [ 1.1276, -1.2876, -0.1206,  0.3778],\n",
       "        [ 0.8339,  0.4186, -0.1903,  0.1530],\n",
       "        [ 0.2991,  0.4172,  1.7093, -1.4071],\n",
       "        [-0.4874, -0.3649,  0.2301,  0.6943],\n",
       "        [ 0.1624, -0.0394, -1.4594, -0.9910],\n",
       "        [ 1.1305, -0.4197, -1.1037,  0.7183],\n",
       "        [-2.1005, -1.1738,  1.2427, -0.3081],\n",
       "        [ 0.7662, -0.9277,  0.1768,  0.5417],\n",
       "        [-1.2029,  0.3766, -1.4913,  1.4805],\n",
       "        [-0.2999,  1.4242, -1.2593, -0.0084],\n",
       "        [-0.3799,  0.5929, -2.0404,  0.9850],\n",
       "        [-0.0511,  0.0799,  0.4022,  0.3626],\n",
       "        [ 0.4143, -0.2962,  0.6588, -0.6158],\n",
       "        [-0.5252, -0.6430,  1.1812, -0.0118],\n",
       "        [ 0.0156, -1.0128,  0.2280, -0.8205],\n",
       "        [ 0.9975,  0.2140,  0.7673,  0.4309]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb3(torch.arange(32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_k = 64\n",
    "d_v = 64\n",
    "torch.manual_seed(44)\n",
    "Q = torch.randn(1, d_k)\n",
    "K = torch.randn(1, d_k)\n",
    "V = torch.randn(1, d_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = ((Q * K.T) / d_k**-0.5 ) @ V.permute(1,0)\n",
    "self_attention = torch.softmax(val, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 64\n",
    "head_size = 16\n",
    "block_size =32\n",
    "dropout = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "def get_vals(n_embd:int=64, head_size:int=16, batch_size:int=32):\n",
    "    key = torch.randn(head_size, batch_size, n_embd)\n",
    "    query = torch.randn(head_size, batch_size, n_embd)\n",
    "    value = torch.randn(head_size, batch_size, n_embd)\n",
    "    return key, query, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size: int=16):\n",
    "        super().__init__()\n",
    "        self.key = torch.randn(n_embd, head_size)\n",
    "        self.query = torch.randn(n_embd, head_size)\n",
    "        self.value = torch.randn(n_embd, head_size)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tril = torch.tril(torch.ones(block_size, block_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randint(7, (3,4,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_attn(q,k,v):\n",
    "    B,T,C = k.shape\n",
    "    wei = q @ k.transpose(-2,-1) * C**-0.5\n",
    "    # is_casual\n",
    "    wei = wei.masked_fill(tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "    # softmax\n",
    "    wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "    # perform the weighted aggregation of the values\n",
    "    # dropout\n",
    "    wei = torch.dropout(wei, dropout, train=True)\n",
    "    # v (B,T,C)\n",
    "    out = wei @ v\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "K, Q, V = get_vals()\n",
    "man = manual_attn(Q,K,V)\n",
    "# k[0, 0, :5], q[0, 0, :5], v[0,0, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.isclose(F.scaled_dot_product_attention(\n",
    "    query=Q, value=V, key=K, is_causal=True), manual_attn(Q, K,V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "K, Q, V = get_vals()\n",
    "auto = F.scaled_dot_product_attention(\n",
    "    query=Q, value=V, key=K, is_causal=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 1.9849e-01, -4.1676e-04,  4.5094e-01,  1.0404e-01, -4.5471e-01,\n",
       "          5.7427e-01,  2.3268e-01,  7.0840e-03,  8.1867e-02, -2.5917e-01,\n",
       "          6.7024e-02, -3.2678e-02, -3.6319e-03,  4.2177e-01, -3.8028e-02,\n",
       "         -6.6800e-02,  1.6193e-01, -3.3688e-02, -8.1977e-02, -9.1186e-02,\n",
       "         -6.3470e-02, -6.5897e-02, -1.6090e-01,  2.0829e-01, -2.1415e-01,\n",
       "          1.3640e-01, -2.6511e-01, -2.9911e-01, -1.1614e-01,  3.5740e-01,\n",
       "         -1.5171e-01,  1.0641e-01, -5.2321e-02,  3.2750e-01,  1.5944e-01,\n",
       "          4.4832e-01, -8.0178e-02,  1.1721e-01,  2.4602e-01, -4.6988e-01,\n",
       "         -2.9049e-01, -2.3800e-02, -4.8197e-02, -3.0183e-01, -7.0186e-02,\n",
       "          3.0087e-01, -3.7347e-01, -2.9339e-01, -7.2708e-02, -8.9672e-03,\n",
       "         -2.2647e-02,  2.9277e-01,  1.9223e-01, -2.9739e-01, -1.5240e-01,\n",
       "         -6.5217e-02, -4.3492e-01,  2.8869e-01, -1.7754e-01,  4.4380e-01,\n",
       "         -1.4488e-01, -3.1673e-01, -4.4581e-02, -2.0972e-01]),\n",
       " tensor([ 0.2300, -0.0146,  0.4259,  0.1289, -0.4049,  0.5005,  0.3001,  0.0278,\n",
       "          0.1086, -0.2267,  0.1315, -0.0297, -0.0620,  0.3551, -0.0492, -0.0083,\n",
       "          0.1778, -0.0987, -0.0255, -0.1461, -0.0653,  0.0013, -0.1046,  0.1569,\n",
       "         -0.2175,  0.0865, -0.1192, -0.2765, -0.0359,  0.3824, -0.0730,  0.0330,\n",
       "         -0.1429,  0.2460,  0.1067,  0.3567, -0.0308,  0.0704,  0.2365, -0.3558,\n",
       "         -0.3771, -0.1110, -0.0166, -0.2874, -0.0237,  0.3822, -0.2482, -0.2294,\n",
       "         -0.0800,  0.0596,  0.0259,  0.2250,  0.2282, -0.2605, -0.1699, -0.0727,\n",
       "         -0.4452,  0.2098, -0.2559,  0.3729, -0.0773, -0.3134, -0.0804, -0.2485]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto[0,-3], man[0,-3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the transformer model with Pytorch in built layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 16  # how many independent sequences will we process in parallel?\n",
    "block_size = 32  # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.210497 M parameters\n",
      "step 0: train loss 4.3350, val loss 4.3303\n",
      "step 100: train loss 2.5679, val loss 2.5770\n",
      "step 200: train loss 0.5630, val loss 0.6058\n",
      "step 300: train loss 0.1930, val loss 0.2117\n",
      "step 400: train loss 0.1221, val loss 0.1265\n",
      "step 500: train loss 0.1031, val loss 0.1059\n",
      "step 600: train loss 0.0944, val loss 0.0979\n",
      "step 700: train loss 0.0900, val loss 0.0926\n",
      "step 800: train loss 0.0889, val loss 0.0896\n",
      "step 900: train loss 0.0847, val loss 0.0866\n",
      "step 1000: train loss 0.0846, val loss 0.0843\n",
      "step 1100: train loss 0.0846, val loss 0.0861\n",
      "step 1200: train loss 0.0827, val loss 0.0807\n",
      "step 1300: train loss 0.0786, val loss 0.0808\n",
      "step 1400: train loss 0.0797, val loss 0.0811\n",
      "step 1500: train loss 0.0774, val loss 0.0788\n",
      "step 1600: train loss 0.0770, val loss 0.0793\n",
      "step 1700: train loss 0.0771, val loss 0.0781\n",
      "step 1800: train loss 0.0777, val loss 0.0765\n",
      "step 1900: train loss 0.0771, val loss 0.0783\n",
      "step 2000: train loss 0.0767, val loss 0.0778\n",
      "step 2100: train loss 0.0764, val loss 0.0763\n",
      "step 2200: train loss 0.0756, val loss 0.0764\n",
      "step 2300: train loss 0.0758, val loss 0.0755\n",
      "step 2400: train loss 0.0744, val loss 0.0745\n",
      "step 2500: train loss 0.0752, val loss 0.0746\n",
      "step 2600: train loss 0.0734, val loss 0.0749\n",
      "step 2700: train loss 0.0731, val loss 0.0744\n",
      "step 2800: train loss 0.0736, val loss 0.0738\n",
      "step 2900: train loss 0.0731, val loss 0.0733\n",
      "step 3000: train loss 0.0715, val loss 0.0746\n",
      "step 3100: train loss 0.0720, val loss 0.0730\n",
      "step 3200: train loss 0.0715, val loss 0.0753\n",
      "step 3300: train loss 0.0726, val loss 0.0731\n",
      "step 3400: train loss 0.0735, val loss 0.0731\n",
      "step 3500: train loss 0.0701, val loss 0.0726\n",
      "step 3600: train loss 0.0705, val loss 0.0727\n",
      "step 3700: train loss 0.0718, val loss 0.0738\n",
      "step 3800: train loss 0.0690, val loss 0.0721\n",
      "step 3900: train loss 0.0688, val loss 0.0731\n",
      "step 4000: train loss 0.0688, val loss 0.0705\n",
      "step 4100: train loss 0.0679, val loss 0.0688\n",
      "step 4200: train loss 0.0677, val loss 0.0705\n",
      "step 4300: train loss 0.0691, val loss 0.0716\n",
      "step 4400: train loss 0.0677, val loss 0.0684\n",
      "step 4500: train loss 0.0682, val loss 0.0697\n",
      "step 4600: train loss 0.0684, val loss 0.0694\n",
      "step 4700: train loss 0.0683, val loss 0.0714\n",
      "step 4800: train loss 0.0665, val loss 0.0690\n",
      "step 4900: train loss 0.0661, val loss 0.0688\n",
      "step 4999: train loss 0.0680, val loss 0.0700\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "M\n",
      "\n",
      "\n",
      "M\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "M\n",
      "M\n",
      "\n",
      "\n",
      "\n",
      "M\n",
      "\n",
      "Hirened?\n",
      "\n",
      "AUMencrver-ansted?\n",
      "\n",
      "Tauss:\n",
      "Wabthy, us crorvetby, delaysayesswey,\n",
      "Oure, not comzorouge outs, tofutste bett!\n",
      "Whed,\n",
      "Ne,\n",
      "es iree-sen cintlat Heaclrovets, and\n",
      "Whe nomy is wabs!\n",
      "elplind,\n",
      "Thou thus, cecriry, that neesplewty.\n",
      "\n",
      "But,USINGBETITINUS\n",
      "Is they, swould thake oul---o what pricksee.\n",
      "\n",
      "Compry, ETENTIO:\n",
      "Bust so mowe-pore\n",
      "That danteruptef so;; wor the: my,\n",
      "I male out fear Prred my om.\n",
      "\n",
      "HKINGLER:\n",
      "Toydy,-arardste they, hest hoin cour aye tey, ry,\n",
      "smy frouf veay, neeans, bemary,\n",
      "Tor coot steam so a theyse.-\n",
      "And, betwerten.\n",
      "\n",
      "CORY; HOzes in so not us bome.TITI\n",
      "cre feomy shoutcas lome\n",
      "thiass, that whes. seaedpaed shealy,\n",
      "Larddparsce oudsss. I vet soye so not us of feRplates:\n",
      "Wher, pongscest?\n",
      "\n",
      "RUpus duched,,\n",
      "To may,\n",
      "younstbeakes aswercerumess my  o with selsly made so lacenty\n",
      "To yourcreadens,\n",
      "Alldemsturdy, brongen, druedtey\n",
      "seot caty, tiued,\n",
      "To not the sut of apprtanou what that that salepe Bloolbke, peas hhat the abade wat do ive cout\n",
      "Sry thu;\n",
      "youkgeg, meees-yay, you pereale,\n",
      "Weet snea;e euchen that be,d, prave,\n",
      "and, and thay no surary, se, messevet,\n",
      "seake ears ying, fie-veve, gre Weove-ved,\n",
      "Dy,\n",
      "The beave! absy seaInersteadeatiet com-ay,\n",
      "Or you benket, dol menttsely tre thirg,\n",
      "Ow, nhis wharweye wo I miemt, so thoud's\n",
      "Wer whertmperulelsloud, the of theest,\n",
      "Erame franeptsrmurther crour, masbe love shy.;\n",
      "\n",
      "Bubslot youst, hot quy:\n",
      "your fsubsed sey, cavenby, no to that theabbe;\n",
      "I not, my hit youu to my whee te you thel!\n",
      "Heanss yeey,\n",
      "And arong ulart erepensm suomes\n",
      "Heouke they wars, evas; fort, thouk\n",
      "--atscees:\n",
      "Ou! proueted,\n",
      "Ourbsed wer rirt arove my ace preiry\n",
      "Hamy, try, they thing, the! romse-mamr\n",
      "Wher arotl,-sarut, gayt, feate you hir,\n",
      "Amaaas our;\n",
      "\n",
      "That paes that lharn our-phat fafly,\n",
      "Burey that heacry,-srofes, of munge,\n",
      "To lareashey at repe what faiopars, I prat\n",
      "burory honeroyer life he puos foopr---porsscearour'st thou.\n",
      "\n",
      "Ruckedery, momy,\n",
      "They, wonsent, Buarbyty, you stan,\n",
      "The sheparfuyermury, and whove.\n",
      "\n",
      "MathmeFss mest, my hen be ou\n",
      "Maru\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "# encoder: take a string, output a list of integers\n",
    "def encode(s): return [stoi[c] for c in s]\n",
    "# decoder: take a list of integers, output a string\n",
    "def decode(l): return ''.join([itos[i] for i in l])\n",
    "\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data))  # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        self.sa = nn.MultiheadAttention(embed_dim=n_embd, batch_first=True, dropout=dropout, num_heads=n_head)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        sa, _ = self.sa(query=self.ln1(x), key=self.ln1(x),\n",
    "                        value=self.ln1(x), need_weights=False, is_causal=False)\n",
    "        x = x + sa\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# super simple bigram model\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)  # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(\n",
    "            torch.arange(T, device=device))  # (T,C)\n",
    "        x = tok_emb + pos_emb  # (B,T,C)\n",
    "        x = self.blocks(x)  # (B,T,C)\n",
    "        x = self.ln_f(x)  # (B,T,C)\n",
    "        logits = self.lm_head(x)  # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(\n",
    "            f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "OM\n",
      "\n",
      "OMingegren.\n",
      "Gry,\n",
      "My,--m fassted mae,\n",
      "Amp thht, eerdaur thountlusessise,\n",
      "Nope wellt Ondly.edo my I weut meneman\n",
      "Bursore of you sis is if Loced\n",
      "Thou heant fadaym outway thy cour thant,\n",
      "Thou my that neare, nold-sy, you\n",
      "And shenceadad, not cemaruy, the youreass\n",
      "Burenceadreyy,\n",
      "Tayce, iront aput upoureluldsden.\n",
      "\n",
      "COMINORL:\n",
      "Wee cost da my pary, yous my, thous.\n",
      "\n",
      "DERCNONED:\n",
      "But bouee,\n",
      "And shou my ovesce fhe thip whort\n",
      "Isteny, yourr hor densles fofur\n",
      "As mostrequt cemane bey pel,'s blat.\n",
      "Yuy then fuee-st, my theighat carvay,\n",
      "Orayayayarpyesty, ruesvy, , chonde.\n",
      "\n",
      "Bund,R:\n",
      "I may cor; thy at the: porirty,\n",
      "Thar fror my maty eroyd, eayesslept?\n",
      "\n",
      "EXOLANRY:\n",
      "Fome werseirg,\n",
      "Becldild mane our that eir yreave,\n",
      "Or dyou roue kares with, crussser,-----pe.\n",
      "\n",
      "Cears,\n",
      "Apprepbeyy, andsr mapathty, andeBsaleg,-yery,\n",
      "Thy, laiepe. Neadeir thun in the\n",
      "\n",
      "CLFORTER:\n",
      "Gooveste.---ouy corthat did sto.\n",
      "\n",
      "Py Cere;\n",
      "Nen brumy, the pre ruiss your kty\n",
      "Ore by to Rirh, that jradobreconsts.\n",
      "\n",
      "Theayend,\n",
      " is thressurce maemy, tf sout the\n",
      "fimy, shatt,\n",
      "chequing whars, seouss in thin aro,\n",
      "Theals: at causs: althy are fre aftuy videst,\n",
      "Hropestedu?\n",
      "\n",
      "Thou driressed, ry, tiok'd ahat to mey\n",
      "Fer, har I lome not reary, and fades;\n",
      "Me veaym, whopestn you syou you mareer cacesuy ruor prafor,\n",
      "my radayw, dar fut is the mure peestrende! and my fur:\n",
      "Thoues.a, lefale, preathteor'd,\n",
      "Os whord, what to meere chot thusgcey,\n",
      "But to matud,one peatesen that fiueatent app,\n",
      "Burdpt veot cagatexpouts is atby's ow-buse,\n",
      "Stmate thuly nothoundrealviomnte\n",
      "Her ereary, nots, hattgedd venvess ancestages,\n",
      "Lopre uraste stamper,\n",
      "Thovene bo areacessvy-bry, evemarn cuppltei,\n",
      "Oesst my diry, imy andk, shat durped,\n",
      "Forvedlary,psubiot-y,\n",
      "Sy cryfy, yerved-ed to beeaaassend,\n",
      "Aun the do sere soy fore.ed ner share teabeed,\n",
      "Huttes, therfud pomy,\n",
      "A thatpstte if dead! founce!\n",
      "Burs hicedsty, my,ed orry somant,\n",
      "Yuce thou cios theet butyoud thoustede thou,\n",
      "Or his che sarwie you the lurdenie thusespank,\n",
      "And be of shen conment, dow my;\n",
      "Bue catmy p\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Built in Transformer Encoder Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.210625 M parameters\n",
      "step 0: train loss 4.2621, val loss 4.2666\n",
      "step 100: train loss 2.5958, val loss 2.6046\n",
      "step 200: train loss 0.6816, val loss 0.7230\n",
      "step 300: train loss 0.1923, val loss 0.2042\n",
      "step 400: train loss 0.1156, val loss 0.1163\n",
      "step 500: train loss 0.0966, val loss 0.0973\n",
      "step 600: train loss 0.0923, val loss 0.0921\n",
      "step 700: train loss 0.0899, val loss 0.0902\n",
      "step 800: train loss 0.0875, val loss 0.0878\n",
      "step 900: train loss 0.0862, val loss 0.0849\n",
      "step 1000: train loss 0.0842, val loss 0.0861\n",
      "step 1100: train loss 0.0856, val loss 0.0856\n",
      "step 1200: train loss 0.0835, val loss 0.0827\n",
      "step 1300: train loss 0.0821, val loss 0.0819\n",
      "step 1400: train loss 0.0824, val loss 0.0823\n",
      "step 1500: train loss 0.0804, val loss 0.0792\n",
      "step 1600: train loss 0.0788, val loss 0.0797\n",
      "step 1700: train loss 0.0797, val loss 0.0804\n",
      "step 1800: train loss 0.0776, val loss 0.0798\n",
      "step 1900: train loss 0.0780, val loss 0.0790\n",
      "step 2000: train loss 0.0783, val loss 0.0780\n",
      "step 2100: train loss 0.0760, val loss 0.0754\n",
      "step 2200: train loss 0.0775, val loss 0.0776\n",
      "step 2300: train loss 0.0767, val loss 0.0789\n",
      "step 2400: train loss 0.0749, val loss 0.0765\n",
      "step 2500: train loss 0.0765, val loss 0.0769\n",
      "step 2600: train loss 0.0751, val loss 0.0751\n",
      "step 2700: train loss 0.0752, val loss 0.0769\n",
      "step 2800: train loss 0.0736, val loss 0.0759\n",
      "step 2900: train loss 0.0730, val loss 0.0748\n",
      "step 3000: train loss 0.0732, val loss 0.0739\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        Block = nn.TransformerEncoderLayer(d_model=n_embd, nhead=n_head, dim_feedforward=4 *\n",
    "                                        n_embd, dropout=dropout, activation='relu', batch_first=True, norm_first=True)\n",
    "        self.blocks = nn.TransformerEncoder(encoder_layer=Block, num_layers=n_layer, norm=nn.LayerNorm(n_embd))\n",
    "        self.ln_f = nn.LayerNorm(n_embd)  # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(\n",
    "            torch.arange(T, device=device))  # (T,C)\n",
    "        x = tok_emb + pos_emb  # (B,T,C)\n",
    "        x = self.blocks(x)  # (B,T,C)\n",
    "        x = self.ln_f(x)  # (B,T,C)\n",
    "        logits = self.lm_head(x)  # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(\n",
    "            f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
