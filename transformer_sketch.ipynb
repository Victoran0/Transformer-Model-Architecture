{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn his model and change the MSA layers to nn.transformerencoder, also, try and change any other layer that is in built in pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Properties from research paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoder: stack N = 6 (identica layers)   \n",
    "dmodel = 512   \n",
    "\n",
    "decoder: stack N = 6 (identica layers)\n",
    "\n",
    "h = 8  \n",
    "dk,dv = 64   \n",
    "\n",
    "hidden layer of ffn = 2048   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(512/8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = nn.Embedding(4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(4, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero =torch.randn(1,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = F.softmax(zero, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7955, 0.0705, 0.0674, 0.0073, 0.0593]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_2 = torch.zeros(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.]]), tensor([[0]]))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_2, mn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb2 = nn.Embedding(65, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = torch.randint(4, (4,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 2, 3, 3, 2, 2, 3, 1],\n",
       "        [3, 2, 3, 0, 3, 3, 1, 1],\n",
       "        [0, 0, 3, 1, 1, 1, 3, 3],\n",
       "        [0, 0, 2, 0, 2, 2, 0, 0]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "mn = torch.multinomial(probs, num_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 2.]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((zero_2, mn), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb3 = nn.Embedding(32, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0548, -0.9601,  0.1128,  0.4235],\n",
       "        [-1.1208, -0.3069,  0.3056,  0.9976],\n",
       "        [ 0.1564,  0.2386, -1.0579, -0.2684],\n",
       "        [-2.8524,  1.3360, -1.2760, -0.4714],\n",
       "        [-0.2928,  0.8955, -0.6687, -1.0177],\n",
       "        [ 0.4759, -0.0696,  0.4178,  0.6019],\n",
       "        [ 0.1742,  1.5600,  1.1127,  1.3638],\n",
       "        [-1.0526,  1.3567,  1.4657,  1.8995],\n",
       "        [ 0.0511,  0.5668, -1.0124, -0.4460],\n",
       "        [-0.5721,  0.4174, -0.1807,  0.1996],\n",
       "        [ 0.8471,  0.4727, -0.2182,  0.9853],\n",
       "        [ 1.5313, -2.3779, -0.2422,  1.1341],\n",
       "        [ 0.3830, -0.3081,  0.7641,  1.3492],\n",
       "        [ 0.3422,  0.4366, -1.5697, -0.3675],\n",
       "        [ 0.0697,  0.1447, -0.3595, -1.4552],\n",
       "        [-0.9424, -0.8859,  0.5325,  0.9046],\n",
       "        [ 1.1276, -1.2876, -0.1206,  0.3778],\n",
       "        [ 0.8339,  0.4186, -0.1903,  0.1530],\n",
       "        [ 0.2991,  0.4172,  1.7093, -1.4071],\n",
       "        [-0.4874, -0.3649,  0.2301,  0.6943],\n",
       "        [ 0.1624, -0.0394, -1.4594, -0.9910],\n",
       "        [ 1.1305, -0.4197, -1.1037,  0.7183],\n",
       "        [-2.1005, -1.1738,  1.2427, -0.3081],\n",
       "        [ 0.7662, -0.9277,  0.1768,  0.5417],\n",
       "        [-1.2029,  0.3766, -1.4913,  1.4805],\n",
       "        [-0.2999,  1.4242, -1.2593, -0.0084],\n",
       "        [-0.3799,  0.5929, -2.0404,  0.9850],\n",
       "        [-0.0511,  0.0799,  0.4022,  0.3626],\n",
       "        [ 0.4143, -0.2962,  0.6588, -0.6158],\n",
       "        [-0.5252, -0.6430,  1.1812, -0.0118],\n",
       "        [ 0.0156, -1.0128,  0.2280, -0.8205],\n",
       "        [ 0.9975,  0.2140,  0.7673,  0.4309]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb3(torch.arange(32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_k = 64\n",
    "d_v = 64\n",
    "torch.manual_seed(44)\n",
    "Q = torch.randn(1, d_k)\n",
    "K = torch.randn(1, d_k)\n",
    "V = torch.randn(1, d_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = ((Q * K.T) / d_k**-0.5 ) @ V.permute(1,0)\n",
    "self_attention = torch.softmax(val, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 64\n",
    "head_size = 16\n",
    "block_size =32\n",
    "dropout = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "def get_vals(n_embd:int=64, head_size:int=16, batch_size:int=32):\n",
    "    key = torch.randn(head_size, batch_size, n_embd)\n",
    "    query = torch.randn(head_size, batch_size, n_embd)\n",
    "    value = torch.randn(head_size, batch_size, n_embd)\n",
    "    return key, query, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size: int=16):\n",
    "        super().__init__()\n",
    "        self.key = torch.randn(n_embd, head_size)\n",
    "        self.query = torch.randn(n_embd, head_size)\n",
    "        self.value = torch.randn(n_embd, head_size)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tril = torch.tril(torch.ones(block_size, block_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randint(7, (3,4,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_attn(q,k,v):\n",
    "    B,T,C = k.shape\n",
    "    wei = q @ k.transpose(-2,-1) * C**-0.5\n",
    "    # is_casual\n",
    "    wei = wei.masked_fill(tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "    # softmax\n",
    "    wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "    # perform the weighted aggregation of the values\n",
    "    # dropout\n",
    "    wei = torch.dropout(wei, dropout, train=True)\n",
    "    # v (B,T,C)\n",
    "    out = wei @ v\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "K, Q, V = get_vals()\n",
    "man = manual_attn(Q,K,V)\n",
    "# k[0, 0, :5], q[0, 0, :5], v[0,0, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.isclose(F.scaled_dot_product_attention(\n",
    "    query=Q, value=V, key=K, is_causal=True), manual_attn(Q, K,V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "K, Q, V = get_vals()\n",
    "auto = F.scaled_dot_product_attention(\n",
    "    query=Q, value=V, key=K, is_causal=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 1.9849e-01, -4.1676e-04,  4.5094e-01,  1.0404e-01, -4.5471e-01,\n",
       "          5.7427e-01,  2.3268e-01,  7.0840e-03,  8.1867e-02, -2.5917e-01,\n",
       "          6.7024e-02, -3.2678e-02, -3.6319e-03,  4.2177e-01, -3.8028e-02,\n",
       "         -6.6800e-02,  1.6193e-01, -3.3688e-02, -8.1977e-02, -9.1186e-02,\n",
       "         -6.3470e-02, -6.5897e-02, -1.6090e-01,  2.0829e-01, -2.1415e-01,\n",
       "          1.3640e-01, -2.6511e-01, -2.9911e-01, -1.1614e-01,  3.5740e-01,\n",
       "         -1.5171e-01,  1.0641e-01, -5.2321e-02,  3.2750e-01,  1.5944e-01,\n",
       "          4.4832e-01, -8.0178e-02,  1.1721e-01,  2.4602e-01, -4.6988e-01,\n",
       "         -2.9049e-01, -2.3800e-02, -4.8197e-02, -3.0183e-01, -7.0186e-02,\n",
       "          3.0087e-01, -3.7347e-01, -2.9339e-01, -7.2708e-02, -8.9672e-03,\n",
       "         -2.2647e-02,  2.9277e-01,  1.9223e-01, -2.9739e-01, -1.5240e-01,\n",
       "         -6.5217e-02, -4.3492e-01,  2.8869e-01, -1.7754e-01,  4.4380e-01,\n",
       "         -1.4488e-01, -3.1673e-01, -4.4581e-02, -2.0972e-01]),\n",
       " tensor([ 0.2300, -0.0146,  0.4259,  0.1289, -0.4049,  0.5005,  0.3001,  0.0278,\n",
       "          0.1086, -0.2267,  0.1315, -0.0297, -0.0620,  0.3551, -0.0492, -0.0083,\n",
       "          0.1778, -0.0987, -0.0255, -0.1461, -0.0653,  0.0013, -0.1046,  0.1569,\n",
       "         -0.2175,  0.0865, -0.1192, -0.2765, -0.0359,  0.3824, -0.0730,  0.0330,\n",
       "         -0.1429,  0.2460,  0.1067,  0.3567, -0.0308,  0.0704,  0.2365, -0.3558,\n",
       "         -0.3771, -0.1110, -0.0166, -0.2874, -0.0237,  0.3822, -0.2482, -0.2294,\n",
       "         -0.0800,  0.0596,  0.0259,  0.2250,  0.2282, -0.2605, -0.1699, -0.0727,\n",
       "         -0.4452,  0.2098, -0.2559,  0.3729, -0.0773, -0.3134, -0.0804, -0.2485]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto[0,-3], man[0,-3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the transformer model with Pytorch in built layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 16  # how many independent sequences will we process in parallel?\n",
    "block_size = 32  # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.210497 M parameters\n",
      "step 0: train loss 4.3350, val loss 4.3303\n",
      "step 100: train loss 2.5679, val loss 2.5770\n",
      "step 200: train loss 0.5630, val loss 0.6058\n",
      "step 300: train loss 0.1930, val loss 0.2117\n",
      "step 400: train loss 0.1221, val loss 0.1265\n",
      "step 500: train loss 0.1031, val loss 0.1059\n",
      "step 600: train loss 0.0944, val loss 0.0979\n",
      "step 700: train loss 0.0900, val loss 0.0926\n",
      "step 800: train loss 0.0889, val loss 0.0896\n",
      "step 900: train loss 0.0847, val loss 0.0866\n",
      "step 1000: train loss 0.0846, val loss 0.0843\n",
      "step 1100: train loss 0.0846, val loss 0.0861\n",
      "step 1200: train loss 0.0827, val loss 0.0807\n",
      "step 1300: train loss 0.0786, val loss 0.0808\n",
      "step 1400: train loss 0.0797, val loss 0.0811\n",
      "step 1500: train loss 0.0774, val loss 0.0788\n",
      "step 1600: train loss 0.0770, val loss 0.0793\n",
      "step 1700: train loss 0.0771, val loss 0.0781\n",
      "step 1800: train loss 0.0777, val loss 0.0765\n",
      "step 1900: train loss 0.0771, val loss 0.0783\n",
      "step 2000: train loss 0.0767, val loss 0.0778\n",
      "step 2100: train loss 0.0764, val loss 0.0763\n",
      "step 2200: train loss 0.0756, val loss 0.0764\n",
      "step 2300: train loss 0.0758, val loss 0.0755\n",
      "step 2400: train loss 0.0744, val loss 0.0745\n",
      "step 2500: train loss 0.0752, val loss 0.0746\n",
      "step 2600: train loss 0.0734, val loss 0.0749\n",
      "step 2700: train loss 0.0731, val loss 0.0744\n",
      "step 2800: train loss 0.0736, val loss 0.0738\n",
      "step 2900: train loss 0.0731, val loss 0.0733\n",
      "step 3000: train loss 0.0715, val loss 0.0746\n",
      "step 3100: train loss 0.0720, val loss 0.0730\n",
      "step 3200: train loss 0.0715, val loss 0.0753\n",
      "step 3300: train loss 0.0726, val loss 0.0731\n",
      "step 3400: train loss 0.0735, val loss 0.0731\n",
      "step 3500: train loss 0.0701, val loss 0.0726\n",
      "step 3600: train loss 0.0705, val loss 0.0727\n",
      "step 3700: train loss 0.0718, val loss 0.0738\n",
      "step 3800: train loss 0.0690, val loss 0.0721\n",
      "step 3900: train loss 0.0688, val loss 0.0731\n",
      "step 4000: train loss 0.0688, val loss 0.0705\n",
      "step 4100: train loss 0.0679, val loss 0.0688\n",
      "step 4200: train loss 0.0677, val loss 0.0705\n",
      "step 4300: train loss 0.0691, val loss 0.0716\n",
      "step 4400: train loss 0.0677, val loss 0.0684\n",
      "step 4500: train loss 0.0682, val loss 0.0697\n",
      "step 4600: train loss 0.0684, val loss 0.0694\n",
      "step 4700: train loss 0.0683, val loss 0.0714\n",
      "step 4800: train loss 0.0665, val loss 0.0690\n",
      "step 4900: train loss 0.0661, val loss 0.0688\n",
      "step 4999: train loss 0.0680, val loss 0.0700\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "M\n",
      "\n",
      "\n",
      "M\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "M\n",
      "M\n",
      "\n",
      "\n",
      "\n",
      "M\n",
      "\n",
      "Hirened?\n",
      "\n",
      "AUMencrver-ansted?\n",
      "\n",
      "Tauss:\n",
      "Wabthy, us crorvetby, delaysayesswey,\n",
      "Oure, not comzorouge outs, tofutste bett!\n",
      "Whed,\n",
      "Ne,\n",
      "es iree-sen cintlat Heaclrovets, and\n",
      "Whe nomy is wabs!\n",
      "elplind,\n",
      "Thou thus, cecriry, that neesplewty.\n",
      "\n",
      "But,USINGBETITINUS\n",
      "Is they, swould thake oul---o what pricksee.\n",
      "\n",
      "Compry, ETENTIO:\n",
      "Bust so mowe-pore\n",
      "That danteruptef so;; wor the: my,\n",
      "I male out fear Prred my om.\n",
      "\n",
      "HKINGLER:\n",
      "Toydy,-arardste they, hest hoin cour aye tey, ry,\n",
      "smy frouf veay, neeans, bemary,\n",
      "Tor coot steam so a theyse.-\n",
      "And, betwerten.\n",
      "\n",
      "CORY; HOzes in so not us bome.TITI\n",
      "cre feomy shoutcas lome\n",
      "thiass, that whes. seaedpaed shealy,\n",
      "Larddparsce oudsss. I vet soye so not us of feRplates:\n",
      "Wher, pongscest?\n",
      "\n",
      "RUpus duched,,\n",
      "To may,\n",
      "younstbeakes aswercerumess my  o with selsly made so lacenty\n",
      "To yourcreadens,\n",
      "Alldemsturdy, brongen, druedtey\n",
      "seot caty, tiued,\n",
      "To not the sut of apprtanou what that that salepe Bloolbke, peas hhat the abade wat do ive cout\n",
      "Sry thu;\n",
      "youkgeg, meees-yay, you pereale,\n",
      "Weet snea;e euchen that be,d, prave,\n",
      "and, and thay no surary, se, messevet,\n",
      "seake ears ying, fie-veve, gre Weove-ved,\n",
      "Dy,\n",
      "The beave! absy seaInersteadeatiet com-ay,\n",
      "Or you benket, dol menttsely tre thirg,\n",
      "Ow, nhis wharweye wo I miemt, so thoud's\n",
      "Wer whertmperulelsloud, the of theest,\n",
      "Erame franeptsrmurther crour, masbe love shy.;\n",
      "\n",
      "Bubslot youst, hot quy:\n",
      "your fsubsed sey, cavenby, no to that theabbe;\n",
      "I not, my hit youu to my whee te you thel!\n",
      "Heanss yeey,\n",
      "And arong ulart erepensm suomes\n",
      "Heouke they wars, evas; fort, thouk\n",
      "--atscees:\n",
      "Ou! proueted,\n",
      "Ourbsed wer rirt arove my ace preiry\n",
      "Hamy, try, they thing, the! romse-mamr\n",
      "Wher arotl,-sarut, gayt, feate you hir,\n",
      "Amaaas our;\n",
      "\n",
      "That paes that lharn our-phat fafly,\n",
      "Burey that heacry,-srofes, of munge,\n",
      "To lareashey at repe what faiopars, I prat\n",
      "burory honeroyer life he puos foopr---porsscearour'st thou.\n",
      "\n",
      "Ruckedery, momy,\n",
      "They, wonsent, Buarbyty, you stan,\n",
      "The sheparfuyermury, and whove.\n",
      "\n",
      "MathmeFss mest, my hen be ou\n",
      "Maru\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "# encoder: take a string, output a list of integers\n",
    "def encode(s): return [stoi[c] for c in s]\n",
    "# decoder: take a list of integers, output a string\n",
    "def decode(l): return ''.join([itos[i] for i in l])\n",
    "\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data))  # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        self.sa = nn.MultiheadAttention(embed_dim=n_embd, batch_first=True, dropout=dropout, num_heads=n_head)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        sa, _ = self.sa(query=self.ln1(x), key=self.ln1(x),\n",
    "                        value=self.ln1(x), need_weights=False, is_causal=False)\n",
    "        x = x + sa\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# super simple bigram model\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)  # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(\n",
    "            torch.arange(T, device=device))  # (T,C)\n",
    "        x = tok_emb + pos_emb  # (B,T,C)\n",
    "        x = self.blocks(x)  # (B,T,C)\n",
    "        x = self.ln_f(x)  # (B,T,C)\n",
    "        logits = self.lm_head(x)  # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(\n",
    "            f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "OM\n",
      "\n",
      "OMingegren.\n",
      "Gry,\n",
      "My,--m fassted mae,\n",
      "Amp thht, eerdaur thountlusessise,\n",
      "Nope wellt Ondly.edo my I weut meneman\n",
      "Bursore of you sis is if Loced\n",
      "Thou heant fadaym outway thy cour thant,\n",
      "Thou my that neare, nold-sy, you\n",
      "And shenceadad, not cemaruy, the youreass\n",
      "Burenceadreyy,\n",
      "Tayce, iront aput upoureluldsden.\n",
      "\n",
      "COMINORL:\n",
      "Wee cost da my pary, yous my, thous.\n",
      "\n",
      "DERCNONED:\n",
      "But bouee,\n",
      "And shou my ovesce fhe thip whort\n",
      "Isteny, yourr hor densles fofur\n",
      "As mostrequt cemane bey pel,'s blat.\n",
      "Yuy then fuee-st, my theighat carvay,\n",
      "Orayayayarpyesty, ruesvy, , chonde.\n",
      "\n",
      "Bund,R:\n",
      "I may cor; thy at the: porirty,\n",
      "Thar fror my maty eroyd, eayesslept?\n",
      "\n",
      "EXOLANRY:\n",
      "Fome werseirg,\n",
      "Becldild mane our that eir yreave,\n",
      "Or dyou roue kares with, crussser,-----pe.\n",
      "\n",
      "Cears,\n",
      "Apprepbeyy, andsr mapathty, andeBsaleg,-yery,\n",
      "Thy, laiepe. Neadeir thun in the\n",
      "\n",
      "CLFORTER:\n",
      "Gooveste.---ouy corthat did sto.\n",
      "\n",
      "Py Cere;\n",
      "Nen brumy, the pre ruiss your kty\n",
      "Ore by to Rirh, that jradobreconsts.\n",
      "\n",
      "Theayend,\n",
      " is thressurce maemy, tf sout the\n",
      "fimy, shatt,\n",
      "chequing whars, seouss in thin aro,\n",
      "Theals: at causs: althy are fre aftuy videst,\n",
      "Hropestedu?\n",
      "\n",
      "Thou driressed, ry, tiok'd ahat to mey\n",
      "Fer, har I lome not reary, and fades;\n",
      "Me veaym, whopestn you syou you mareer cacesuy ruor prafor,\n",
      "my radayw, dar fut is the mure peestrende! and my fur:\n",
      "Thoues.a, lefale, preathteor'd,\n",
      "Os whord, what to meere chot thusgcey,\n",
      "But to matud,one peatesen that fiueatent app,\n",
      "Burdpt veot cagatexpouts is atby's ow-buse,\n",
      "Stmate thuly nothoundrealviomnte\n",
      "Her ereary, nots, hattgedd venvess ancestages,\n",
      "Lopre uraste stamper,\n",
      "Thovene bo areacessvy-bry, evemarn cuppltei,\n",
      "Oesst my diry, imy andk, shat durped,\n",
      "Forvedlary,psubiot-y,\n",
      "Sy cryfy, yerved-ed to beeaaassend,\n",
      "Aun the do sere soy fore.ed ner share teabeed,\n",
      "Huttes, therfud pomy,\n",
      "A thatpstte if dead! founce!\n",
      "Burs hicedsty, my,ed orry somant,\n",
      "Yuce thou cios theet butyoud thoustede thou,\n",
      "Or his che sarwie you the lurdenie thusespank,\n",
      "And be of shen conment, dow my;\n",
      "Bue catmy p\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Built in Transformer Encoder Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.210625 M parameters\n",
      "step 0: train loss 4.2621, val loss 4.2666\n",
      "step 100: train loss 2.5958, val loss 2.6046\n",
      "step 200: train loss 0.6816, val loss 0.7230\n",
      "step 300: train loss 0.1923, val loss 0.2042\n",
      "step 400: train loss 0.1156, val loss 0.1163\n",
      "step 500: train loss 0.0966, val loss 0.0973\n",
      "step 600: train loss 0.0923, val loss 0.0921\n",
      "step 700: train loss 0.0899, val loss 0.0902\n",
      "step 800: train loss 0.0875, val loss 0.0878\n",
      "step 900: train loss 0.0862, val loss 0.0849\n",
      "step 1000: train loss 0.0842, val loss 0.0861\n",
      "step 1100: train loss 0.0856, val loss 0.0856\n",
      "step 1200: train loss 0.0835, val loss 0.0827\n",
      "step 1300: train loss 0.0821, val loss 0.0819\n",
      "step 1400: train loss 0.0824, val loss 0.0823\n",
      "step 1500: train loss 0.0804, val loss 0.0792\n",
      "step 1600: train loss 0.0788, val loss 0.0797\n",
      "step 1700: train loss 0.0797, val loss 0.0804\n",
      "step 1800: train loss 0.0776, val loss 0.0798\n",
      "step 1900: train loss 0.0780, val loss 0.0790\n",
      "step 2000: train loss 0.0783, val loss 0.0780\n",
      "step 2100: train loss 0.0760, val loss 0.0754\n",
      "step 2200: train loss 0.0775, val loss 0.0776\n",
      "step 2300: train loss 0.0767, val loss 0.0789\n",
      "step 2400: train loss 0.0749, val loss 0.0765\n",
      "step 2500: train loss 0.0765, val loss 0.0769\n",
      "step 2600: train loss 0.0751, val loss 0.0751\n",
      "step 2700: train loss 0.0752, val loss 0.0769\n",
      "step 2800: train loss 0.0736, val loss 0.0759\n",
      "step 2900: train loss 0.0730, val loss 0.0748\n",
      "step 3000: train loss 0.0732, val loss 0.0739\n",
      "step 3100: train loss 0.0745, val loss 0.0739\n",
      "step 3200: train loss 0.0740, val loss 0.0737\n",
      "step 3300: train loss 0.0724, val loss 0.0748\n",
      "step 3400: train loss 0.0705, val loss 0.0734\n",
      "step 3500: train loss 0.0708, val loss 0.0727\n",
      "step 3600: train loss 0.0718, val loss 0.0735\n",
      "step 3700: train loss 0.0707, val loss 0.0715\n",
      "step 3800: train loss 0.0701, val loss 0.0710\n",
      "step 3900: train loss 0.0709, val loss 0.0732\n",
      "step 4000: train loss 0.0721, val loss 0.0726\n",
      "step 4100: train loss 0.0713, val loss 0.0719\n",
      "step 4200: train loss 0.0709, val loss 0.0722\n",
      "step 4300: train loss 0.0727, val loss 0.0729\n",
      "step 4400: train loss 0.0721, val loss 0.0716\n",
      "step 4500: train loss 0.0696, val loss 0.0707\n",
      "step 4600: train loss 0.0685, val loss 0.0715\n",
      "step 4700: train loss 0.0709, val loss 0.0717\n",
      "step 4800: train loss 0.0719, val loss 0.0715\n",
      "step 4900: train loss 0.0690, val loss 0.0712\n",
      "step 4999: train loss 0.0679, val loss 0.0707\n",
      "\n",
      "RRRRRRRRRRRRRRRRRRRORRRRRRRRRRRRRRDORGRORD:\n",
      "And a mothetles he hund soi is ishen ohe thhe didcy\n",
      "llin, qhe cand unth bunstaul noi,\n",
      "And hath ha int lost oCot shish be a hes thou;\n",
      "Ho a:\n",
      "Thit urfhom oson!' bnthap\n",
      "Che dand dowe I eowreentlre I sheankyer\n",
      "I Laines hy momben:\n",
      "sent tre soncrast lere, sourfilt!\n",
      "Tu stun soumt cresemestopo drenghir,\n",
      "The here mell muns resd bere, houle plank he file meseve\n",
      "foile's wham cmanlind rerl\n",
      "And donth they I th prese, her alruld\n",
      "'the be'go,\n",
      "Thant ear on Her is Hhis rrry, a soo wreveve\n",
      "Nar my pavek hinlo fibe so be, whil of\n",
      "sin iugh youl gerese he there it you\n",
      "re o  us widt: thesh, me 'S: bids,;\n",
      "\n",
      "Herip than nerife roth:\n",
      "\n",
      "LlERHB:\n",
      "Cum, werere bary gooch foreronk's;\n",
      "Dhardatinblukd doinour id that. hy sorenre storir'seile vejn,\n",
      "yes heene, we: sutwigh ma tof uticheor nith'Fowa thewer liy.\n",
      "\n",
      "POlEEDEKEUD:\n",
      "Teprave RiI feriy, thelp mee ise.\n",
      "Domy kra ind Verifry my soMm! dinsh,\n",
      "I hen drers he the throing het hisghy\n",
      "Me Le,\n",
      "Yike on the ble sitile, siplidht ay eray,\n",
      "\n",
      "MEULENhis Yestry so his bit! be hee!\n",
      "scocthat: you the thea seefe.\n",
      "Mype you slest:\n",
      "Pok'll nhs tyour werbesters, lienelo a\n",
      "Ror ceret lait:\n",
      "Mya not pe Fonoy oues ceseco bontafear, yon;\n",
      "I me this reare af hor clwerd, to\n",
      "Thirh iner that mis gonrring oworu by a\n",
      "pe be nend hive\n",
      "Thy wles she; unthat to in theppraa bf'y.\n",
      "\n",
      "DUTIUMEDULENE:\n",
      "Doors nonoys, noth wigh esth thalp ons\n",
      "fothe cough ofinghthim oromo hillien:\n",
      "Theu lolodtould blorkelf blot:\n",
      "ho the\n",
      "\n",
      "ULOR:\n",
      "Doow'se th' ce vinve the hitwew\n",
      "D yo sromunesor.\n",
      "Shisulh:\n",
      "Surungendo st by hes sane thee bu thisg shene band thinhts:\n",
      "Darrs butthey!\n",
      "I shing tha se ghthey oundigher\n",
      "of yone frarh go s bro, thern tonty,\n",
      "Thim h; amangy\n",
      "\n",
      "\n",
      "Bet in bithteror deonr whhe by dilbe.\n",
      "\n",
      "EUIUS:\n",
      "Rururk ches do stillfoul there cirrbe of I:\n",
      "Cmive is this oudert bye by he mod.\n",
      "\n",
      "ICM: hase leon the hilsseam his;\n",
      "Lod tee'be hear he\n",
      "My dhiow's her thith miok onds dres thre.\n",
      "\n",
      "Dauno! bhathy, thy dow mane mamy\n",
      "hel hetth ann went apthe bornpe-hirrilb a bbreny.\n",
      "O, feittithins; your neis t\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        Block = nn.TransformerEncoderLayer(d_model=n_embd, nhead=n_head, dim_feedforward=4 *\n",
    "                                        n_embd, dropout=dropout, activation='relu', batch_first=True, norm_first=True)\n",
    "        self.blocks = nn.TransformerEncoder(encoder_layer=Block, num_layers=n_layer, norm=nn.LayerNorm(n_embd), enable_nested_tensor=False)\n",
    "        self.ln_f = nn.LayerNorm(n_embd)  # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(\n",
    "            torch.arange(T, device=device))  # (T,C)\n",
    "        x = tok_emb + pos_emb  # (B,T,C)\n",
    "        x = self.blocks(x)  # (B,T,C)\n",
    "        x = self.ln_f(x)  # (B,T,C)\n",
    "        logits = self.lm_head(x)  # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(\n",
    "            f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('newsdata.json', 'r', encoding=\"utf8\") as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_arr = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in data[\"results\"]:\n",
    "    if item[\"content\"] is not None and f'{item[\"title\"]}: \\n {item[\"content\"]} \\n' not in data_arr:\n",
    "        data_arr.append(f'{item[\"title\"]}: \\n {item[\"content\"]} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Vance, Walz Hold Dueling CEO Meetings Ahead of VP Debate: \\n (Bloomberg) — The vice presidential nominees courted chief executive officers in dueling meetings in Washington, as the two campaigns seek to win over major business leaders in an exceedingly tight race. The separate meetings on Thursday with Republican JD Vance and Democrat Tim Walz at the corporate-friendly Business Roundtable — a group representing chief executive officers from some of the largest American companies — highlighted how the campaigns of Donald Trump and Kamala Harris are both seeking to bolster their economic credentials with less than 50 days before the election. Executives slated to attend the meetings included Mary Barra of General Motors, Home Depot Inc.’s Ted Decker, Jon Moeller of Procter & Gamble Co. and Cisco Systems Inc.’s Chuck Robbins. The Harris-Walz campaign said in a statement that the Minnesota governor spoke to about 100 CEOs, where he highlighted “Vice President Harris’ commitment to advancing practical, pro-growth, and fiscally responsible economic policies,” drawing “a strong contrast against Donald Trump.” In his meeting, Vance touted Trump’s pledges to lower taxes and argued for the Republican nominee’s tariff policies, according to a person familiar with the conversation who spoke on condition of anonymity to detail the meeting. Trump has vowed to renew expiring tax cuts and lower the corporate tax rate even further, to 15% from 21% — proposals which have won him support from business leaders. But his plans to hit US allies and adversaries alike with trade levies threaten to upend global trade. Mainstream economists argue Trump’s trade agenda could amount to a tax increase on US households, raising prices on a broad range of goods. The person said Vance also discussed energy policy, casting affordable, abundant energy supplies as critical to domestic manufacturing — and highlighting the importance of swing-state Pennsylvania, which is experiencing an energy boom thanks to fracking. Vance also discussed the need for the US to gain an edge in emerging technologies, such as artificial intelligence, according to the person. Vance and Walz are set to take the same stage at an Oct. 1 debate hosted by CBS News, in what could be the final time the presidential or vice presidential candidates appear together before the Nov. 5 election. Since replacing President Joe Biden as the Democratic nominee in July, the Harris campaign has boosted outreach to the business community. Second Gentleman Doug Emhoff has emphasized this message with wealthy donors at a series of fundraisers, calling Harris a “pro-growth” capitalist who understands corporate America’s needs. Trump spoke to the Business Roundtable in June, where he pledged to lower the corporate tax rate and slash federal regulations — two key priorities for the group’s members. Biden, who at the time was the Democratic nominee, was invited to speak, but White House Chief of Staff Jeff Zients spoke in his stead. Harris has taken a softer approach with businesses than Biden, rolling out campaign policy plans that would provide deductions for startups and scaling back the capital gains tax hike plan Biden endorsed. The Business Roundtable is among the many executive and corporate groups who strongly denounced Trump after his supporters sought to overturn the 2020 election. Still, many business leaders and deep-pocketed donors have been dismayed by Biden’s record on inflation, regulations and foreign policy, and they’ve opted to make the bet that a Republican will be a better steward of the economy — even if some privately worry about his temperament or threats of retribution against opponents. —With assistance from Nancy Cook. (Updates with details on Vance meetings in paragraphs five-seven.) \\n',\n",
       " 'Trump vows to be ’best friend’ to Jewish Americans, as allegations of ally’s anti-Semitism surface: \\n WASHINGTON (AP) — Former President Donald Trump on Thursday decried anti-Semitism hours after an explosive CNN report detailed how one of his allies running for North Carolina governor made a series of racial and sexual comments on a website where he also referred to himself as a “black NAZI.” North Carolina Lt. Gov. Mark Robinson vowed to remain in the race despite the report, and the Trump campaign appeared to be distancing itself from the candidate while still calling the battleground state a vital part to winning back the White House. Trump has frequently voiced his support for Robinson, who has been considered a rising star in his party despite a history of inflammatory remarks about race and abortion. Trump did not comment on the allegations during his Thursday addresses to a group of Jewish donors and to the Israeli-American Council in Washington. His campaign issued a statement about the CNN story that did not mention Robinson, saying instead that Trump “is focused on winning the White House and saving this country” and that North Carolina was a “vital part of that plan.” Robinson’s reported remarks — including a 2012 comment in which he said he preferred Adolf Hitler to the leadership in Washington _ clashed with Trump’s denunciations of anti-Semitism in Washington and his claim that Vice President Kamala Harris, the Democratic nominee, sympathized with enemies of Israel. The story also could threaten Trump’s chances of winning North Carolina, a key battleground state, with Robinson already running well behind his Democratic opponent in public polls. “This story is not about the governor’s race in North Carolina. It’s about the presidential race,” said Paul Shumaker, a Republican pollster who’s worked for Sen. Thom Tillis, R-N.C., and warned that Trump could risk losing a state he won in 2016 and 2020. “The question is going to be, does Mark Robinson cost Donald Trump the White House?” Shumaker added. After allegations against Robinson became public, a spokesman for Harris’ campaign, Ammar Moussa, reposted on social media a photo of Trump and the embattled candidate. “Donald Trump has a Mark Robinson problem,” he wrote. The North Carolina Republican Party issued a statement standing by Robinson, noting he “categorically denied the allegations made by CNN but that won’t stop the Left from trying to demonize him via personal attacks.” Trump has angled to make inroads among Black voters and frequently aligned himself with Robinson along the campaign trail, which has more and more frequently taken him to North Carolina. At a rally in Greensboro, he called Robinson “Martin Luther King on steroids” in reference to the civil rights leader, for his speaking ability. Robinson has been on the trail with Trump as recently as last month, when he appeared with the GOP nominee at an event in Asheboro, North Carolina. Recent polls of North Carolina voters show Trump and Harris locked in a close race. The same polls show Democrat Josh Stein with a roughly 10-point lead over Robinson. Both Trump and Harris, the Democratic nominee, were making appearances meant to fire up their core supporters, with Harris participating in a livestream with Oprah Winfrey. Trump appeared Thursday with Miriam Adelson, a co-owner of the NBA’s Dallas Mavericks and widow of billionaire casino magnate Sheldon Adelson. “My promise to Jewish Americans is this: With your vote, I will be your defender, your protector, and I will be the best friend Jewish Americans have ever had in the White House,” Trump said during the donor event in Washington, titled “Fighting Anti-Semitism in America.” “But in all fairness, I already am,” Trump added. Trump also has been criticized for his association with extremists who spew anti-Semitic rhetoric such as far-right activist Nick Fuentes and rapper Ye, formerly known as Kanye West. And when former Ku Klux Klan leader David Duke endorsed Trump in 2016, Trump responded in a CNN interview that he knew “nothing about David Duke, I know nothing about white supremacists.” But during his four years in office Trump approved a series of policy changes long sought by many advocates of Israel, such as moving the U.S. embassy from Tel Aviv to Jerusalem and recognizing Israel’s annexation of the Golan Heights. In his remarks, Trump criticized Harris over the Biden administration’s handling of the Israel-Hamas war and for what he called antisemitic protests on college campuses and elsewhere. “Kamala Harris has done absolutely nothing. She has not lifted a single finger to protect you or to protect your children,” Trump said. He also repeated a talking point that Jewish voters who vote for Democrats “should have their head examined.” Multiple attendees at the event said they weren’t familiar with the story about Robinson or declined to discuss it. Rep. Virginia Foxx, a conservative North Carolina Republican who was asked about the CNN report beforehand, told reporters she wasn’t taking questions. Later Thursday, Trump spoke at the Israeli-American Council National Summit to honor the victims of Hamas’ Oct. 7 attack on Israel and painted a dire future for the nation if Harris were to be elected. “Israel will not exist within two years if she becomes president,” he told the crowd, also adding that if he loses the presidential election to her on Nov. 5 “the Jewish people would really have a lot to do with that.” Harris on Thursday faced pressure from parts of her liberal base over the war. Leaders of the Democratic protest vote movement “Uncommitted” said the group would not endorse Harris for president, but also urged supporters to vote against Trump. The group, which opposes the Biden administration’s handling of the Israel-Hamas war, has called for an immediate ceasefire in Gaza and an end to U.S. weapons transfers to Israel. “Uncommitted” drew hundreds of thousands of votes in this year’s Democratic primaries, surfacing a rift within the party. The group has warned that some Democratic voters may stay home in November, particularly in places like Michigan. Harris’ campaign did not directly address the group’s announcement, but said in a statement that she will “continue working to bring the war in Gaza to an end in a way where Israel is secure, the hostages are released, the suffering in Gaza ends, and the Palestinian people can realize their right to dignity, security, freedom and self-determination.” \\n',\n",
       " 'Gun-owner Kamala Harris says \\'they\\'re getting shot\\' if intruder breaks into her house: \\n WASHINGTON — Vice President Kamala Harris said Thursday that any intruder who breaks into her home is \"getting shot,\" a comment that seemed to be made in jest during an interview with Oprah Winfrey . Winfrey, while hosting a star-studded livestream town hall with the Democratic nominee, told Harris in an interview that she was surprised to hear during last week\\'s presidential debate with Donald Trump that she is a gun owner . \"If somebody breaks into my house, they\\'re getting shot,\" Harris responded, then broke into a laugh. \"Probably should not have said that. My staff will deal with that later.\" More: Kamala Harris said she won’t take Americans\\' guns away. Because she owns one too. As vice president, Harris resides in the heavily protected Naval Observatory residence in Washington. Sign-up for Your Vote: Text with the USA TODAY elections team. The exchange came during a discussion on gun control. Harris supports strengthening background checks to purchase guns and reinstating a federal assault weapons ban. \"Here\\'s my point, Oprah. I\\'m not trying to take everyone\\'s guns away. I believe in the Second Amendment,\" Harris said. \"These are just common sense,\" she said of the gun control measures she supports. Harris recounted holding town halls at colleges last year in which she would ask the students to raise their hands if at any point between kindergarten and 12th grade they had to take part in an active shooter drill. \"Almost every hand went up. It was bone chilling,\" Harris said. Reach Joey Garrison on X, formerly Twitter, @joeygarrison. \\n',\n",
       " 'AI-generated fake endorsements: Taylor Swift, Lady Gaga...used in US 2024 election misinformation: \\n Washington: Taylor Swift did not endorse Donald Trump. Nor did Lady Gaga or Morgan Freeman. And Bruce Springsteen was not photographed in a \"Keep America Trumpless\" shirt. Fake celebrity endorsements and snubs are roiling the US presidential race. Dozens of bogus testimonies from American actors, singers and athletes about Republican nominee Trump and his Democratic rival Kamala Harris have proliferated on social media ahead of the November election, researchers say, many of them enabled by artificial intelligence image generators. The fake endorsements and brushoffs, which come as platforms such as the Elon Musk-owned X knock down many of the guardrails against misinformation, have prompted concern over their potential to manipulate voters as the race to the White House heats up. Last month, Trump shared doctored images showing Swift threw her support behind his campaign, apparently seeking to tap into the pop singer\\'s mega star power to sway voters. The photos -- including some that Hany Farid, a digital forensics expert at the University of California, Berkeley, said bore the hallmarks of AI-generated images -- suggested the pop star and her fans, popularly known as Swifties, backed Trump\\'s campaign. What made Trump\\'s mash-up on Truth Social \"particularly devious\" was its combination of real and fake imagery, Farid told AFP. Last week, Swift endorsed Harris and her running mate Tim Walz, calling the current vice president a \"steady-handed, gifted leader.\" The singer, who has hundreds of millions of followers on platforms including Instagram and TikTok, said those manipulated images of her motivated her to speak up as they \"conjured up my fears around AI, and the dangers of spreading misinformation.\" Following her announcement, Trump fired a missive on Truth Social saying: \"I HATE TAYLOR SWIFT!\" A database from the News Literacy Project (NLP), a nonprofit which recently launched a misinformation dashboard to raise awareness about election falsehoods, has so far listed 70 social media posts peddling fake \"VIP\" endorsements and snubs. \"In these polarizing times, fake celebrity endorsements can grab voters\\' attention, influence their outlooks, confirm personal biases, and sow confusion and chaos,\" Peter Adams, senior vice president for research at NLP, told AFP. NLP\\'s list, which appears to be growing by the day, includes viral posts that have garnered millions of views. Among them are posts sharing a manipulated picture of Lady Gaga with a \"Trump 2024\" sign, implying that she endorsed the former president, AFP\\'s fact-checkers reported. Other posts falsely asserted that the Oscar-winning Morgan Freeman, who has been critical of the Republican, said that a second Trump presidency would be \"good for the country,\" according to US fact-checkers. Digitally altered photos of Springsteen wearing a \"Keep America Trumpless\" shirt and actor Ryan Reynolds sporting a \"Kamala removes nasty orange stains\" shirt also swirled on social media sites. \"The platforms have enabled it,\" Adams said. \"As they pull back from moderation and hesitate to take down election related misinformation, they have become a major avenue for trolls, opportunists and propagandists to reach a mass audience.\" In particular, X has emerged as a hotbed of political disinformation after the platform scaled back content moderation policies and reinstated accounts of known purveyors of falsehoods, researchers say. Musk, who has endorsed Trump and has over 198 million followers on X, has been repeatedly accused of spreading election falsehoods. American officials responsible for overseeing elections have also urged Musk to fix X\\'s AI chatbot known as Grok -- which allows users to generate AI-generated images from text prompts -- after it shared misinformation. Lucas Hansen, co-founder of the nonprofit CivAI, demonstrated to AFP the ease with which Grok can generate a fake photo of Swift fans supporting Trump using a simple prompt: \"Image of an outside rally of woman wearing \\'Swifties for Trump\\' T-shirts.\" \"If you want a relatively mundane situation where the people in the image are either famous or fictitious, Grok is definitely a big enabler\" of visual disinformation, Hansen told AFP. \"I do expect it to be a large source of fake celebrity endorsement images,\" he added. As the technology develops, it\\'s going to become \"harder and harder to identify the fakes,\" said Jess Terry, Intelligence Analyst at Blackbird.AI. \"There\\'s certainly the risk that older generations or other communities less familiar with developing AI-based technology might believe what they see,\" Terry told AFP. \\n',\n",
       " 'Trump bemoans lack of support from Jewish voters and blames ‘Democrat curse’: \\n Donald Trump has complained bitterly to Jewish donors that a majority of Jews vote against him in US presidential elections, suggesting that the Democratic party has a “curse on you”. The Republican presidential candidate made the remarks during a speech on Thursday at the Israeli-American Council national summit in Washington, where he used hyperbolic language to warn that victory for his opponent Kamala Harris would result in Israel being wiped off the map. Airing grievances at the end of a disjointed speech, with US and Israel flags behind him, Trump claimed that his support among Jewish voters went from 25% in 2016 to 29% in 2020. “And based on what I did and based on my love – the same love that you have – I should be at 100,” he carped. Trump asserted that he had been “the best president by far” for Israel but a new poll shows him still below 40% among Jewish voters. “That means you’ve got 60% voted for somebody that hates Israel. And I say it – it’s going to happen – it’s only because of the Democrat hold or curse on you. You can’t let this happen. Forty percent is not acceptable, because we have an election to win.” Trump has been criticised for associating with extremists who promote antisemitic rhetoric, such as the far-right activist Nick Fuentes and the rapper Ye, formerly known as Kanye West. When the former Ku Klux Klan leader David Duke endorsed Trump in 2016, Trump responded that he knew “nothing about David Duke, I know nothing about white supremacists”. But during his four years in office, Trump approved a series of policy changes long sought by many advocates of Israel, such as moving the US embassy from Tel Aviv to Jerusalem, officially recognising the Golan Heights as being under Israel’s sovereignty, and terminating Barack Obama’s Iran nuclear deal. At Thursday’s donor event, entitled “Fighting Anti-Semitism in America”, Trump told the mostly supportive audience: “My promise to Jewish Americans is this: with your vote I will be your defender, your protector, and I will be the best friend Jewish Americans have ever had in the White House. But in all fairness, I already am.” He criticised Harris over the Biden administration’s handling of the Israel-Hamas war, and for what he branded antisemitic protests on college campuses and elsewhere. “Kamala Harris has done absolutely nothing. She has not lifted a single finger to protect you or to protect your children.” But the former president returned again and again to what is evidently a political sore point: his persistent struggle among Jewish voters. He repeated a talking point that Jewish people who vote for Democrats “should have their head examined”. He went on: “I will put it to you very simply and gently. I really haven’t been treated right. But you haven’t been treated right because you’re putting yourself in great danger and the United States hasn’t been treated right.” He claimed that Israel “will cease to exist” within two or three years if he does not win the election. “I have to tell you the truth and maybe you’ll be energised because there’s no way that I should be getting 40% of the vote. I’m the one that’s protecting you. These are the people who are going destroy you and you have 60% of Jewish people essentially voting for that.” Trump claimed that a recent poll in Israel was 99% favourable towards him, though it was unclear what poll he was citing. He went on to boast : “Everybody loves me. I could run for prime minister but I’d have to learn your language. That’s a tough language to learn ... I’m the most popular person in Israel. But here it doesn’t translate. It is a strange thing.” Concluding his remarks, the former president reiterated: “I believe that Israel will be wiped off the face of the earth if I don’t win.” He described, without evidence, Harris as “anti-Israel” and “anti-Jewish”, even though the vice-president is married to a Jewish man, Doug Emhoff. Trump was introduced by the megadonor Miriam Adelson, a co-owner of the Dallas Mavericks NBA team and the widow of billionaire casino magnate Sheldon Adelson. Critics have likened the Adelsons’ ability to pull public policy on Israel away from public opinion to the National Rifle Association’s influence on gun laws. Miriam Adelson praised Trump’s “beautiful Jewish daughter” Ivanka and urged the gathering to support him. “All of us Jews must vote for him,” she said. “It is our sacred duty in gratitude for everything he has done and trust in everything he will yet do.” Earlier on Thursday, leaders of the Uncommitted Democratic protest vote movement said the group would not endorse Harris for president, but also urged supporters to vote against Trump. The group, which opposes the Biden administration’s handling of the Israel-Hamas war, has called for an immediate ceasefire in Gaza and an end to US weapons transfers to Israel. \\n',\n",
       " 'At DC event, Trump vows to bring back travel ban, bar Gaza refugees from entering US: \\n WASHINGTON — Former President Donald Trump vowed to reinstate his travel ban that barred people from some predominantly Muslim countries and expand it to prevent refugees from war-torn Gaza from entering the United States. “I will ban refugee resettlement from terror-infested areas like the Gaza Strip, and we will seal our border and bring back the travel ban,” Trump said Thursday evening in Washington, D.C., at an event alongside Republican donor and billionaire Miriam Adelson. “Remember the famous travel ban? We didn’t take people from certain areas of the world,” Trump added “We’re not taking them from infested countries.” Trump initially put in place a version of his travel ban — one of the signature measures of his presidency — a week after taking office, triggering chaos at airports and sparking protests. Judges blocked the initial ban but changes to the policy eventually led to it being upheld by the U.S. Supreme Court, which rejected claims that it targeted Muslims. The Republican presidential nominee’s remarks came at an event focused on combating antisemitism, as he sought to enhance his outreach to Jewish voters before November’s election against Vice President Kamala Harris, his Democratic opponent. Throughout the speech, Trump frequently suggested his backing for Israel should result in better political support among Jewish Americans. He repeatedly complained he had not “been treated right” because polls showed a majority of Jewish Americans supported his opponent, and said that he believed “Jewish people would have a lot to do with a loss” in the presidential election. “There’s no way that I should be getting 40% of the vote — I’m the one who is protecting you,” Trump said. Trump at one point suggested that Israel itself should defeat Harris. “More than any people on Earth, Israel has to defeat her,” he said. The former president added that Israel’s very existence could hinge on the election: “If I don’t win, I believe Israel will be eradicated,” he said. Trump has repeatedly come under criticism for remarks asserting that American Jews ought to be unquestioningly supportive of the Israeli government. In an interview in March, Trump accused Jewish people who support Democrats of hating their religion and Israel. Trump has sought to seize on divisions among Democrats over Israel’s war against Hamas, designated a terrorist group by the U.S. and European Union. In a July radio interview, Trump attacked Harris, saying she “doesn’t like Jewish people” and that she had appeared annoyed during a meeting with Israeli Prime Minister Benjamin Netanyahu. When the radio host criticized Harris’ husband, second gentleman Doug Emhoff, calling him a “crappy Jew,” Trump responded “yeah.” Emhoff is the first Jewish spouse of a U.S. president or vice president and has been a vocal advocate against antisemitism, including leading the administration’s strategy on the issue. After Trump’s speech on Thursday night, Amy Spitalnick, chief executive officer of the Jewish Council for Public Affairs issued a rebuke of his remarks. “Trump continues to label Jews who don’t support him as disloyal and crazy, to play into dangerous dual loyalty tropes, and to blame Jews for a potential electoral loss,” she said. “At the same time, he continues to normalize antisemitic extremism.” Trump has drawn criticism for interactions with antisemites and white supremacists, including a dinner at Mar-a-Lago in November 2022 with Nick Fuentes, a Holocaust denier. A Pew Research Center survey conducted from Aug. 26 to Sept. 2 found 65% of Jewish registered voters support or are leaning toward Harris, with 34% for Trump. The war in Gaza has presented a political challenge for Harris, with progressives and younger voters critical of President Joe Biden’s support for Israel. Harris, while backing Israel’s right to defend itself, has expressed more empathy for Palestinian suffering than Biden during the war. (Bloomberg staff writers Jordan Fabian and Bill Allison contributed to this story.) ©2024 Bloomberg L.P. Visit bloomberg.com. Distributed by Tribune Content Agency, LLC. \\n']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_arr.pop(4)\n",
    "data_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check the length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_str = \"\".join(data_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(data_str)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "# encoder: take a string, output a list of integers\n",
    "def encode(s): return [stoi[c] for c in s]\n",
    "# decoder: take a list of integers, output a string\n",
    "def decode(l): return ''.join([itos[i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([58, 59, 59, 1, 70, 58, 55, 68, 55], 'hii there')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode('hii there'), decode(encode('hii there'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(data_str), dtype=torch.long)\n",
    "n = int(0.9*len(data))  # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.213076 M parameters\n",
      "step 0: train loss 4.5665, val loss 4.5616\n",
      "step 100: train loss 2.6934, val loss 2.7431\n",
      "step 200: train loss 0.8522, val loss 0.8927\n",
      "step 300: train loss 0.2268, val loss 0.2393\n",
      "step 400: train loss 0.1435, val loss 0.1601\n",
      "step 500: train loss 0.1151, val loss 0.1290\n",
      "step 600: train loss 0.1054, val loss 0.1145\n",
      "step 700: train loss 0.0980, val loss 0.1087\n",
      "step 800: train loss 0.0949, val loss 0.1019\n",
      "step 900: train loss 0.0921, val loss 0.1020\n",
      "step 1000: train loss 0.0875, val loss 0.0982\n",
      "step 1100: train loss 0.0883, val loss 0.0977\n",
      "step 1200: train loss 0.0846, val loss 0.0957\n",
      "step 1300: train loss 0.0862, val loss 0.0970\n",
      "step 1400: train loss 0.0816, val loss 0.0935\n",
      "step 1500: train loss 0.0818, val loss 0.0924\n",
      "step 1600: train loss 0.0806, val loss 0.0897\n",
      "step 1700: train loss 0.0807, val loss 0.0907\n",
      "step 1800: train loss 0.0801, val loss 0.0900\n",
      "step 1900: train loss 0.0802, val loss 0.0890\n",
      "step 2000: train loss 0.0771, val loss 0.0891\n",
      "step 2100: train loss 0.0761, val loss 0.0888\n",
      "step 2200: train loss 0.0753, val loss 0.0844\n",
      "step 2300: train loss 0.0759, val loss 0.0875\n",
      "step 2400: train loss 0.0761, val loss 0.0851\n",
      "step 2500: train loss 0.0759, val loss 0.0845\n",
      "step 2600: train loss 0.0750, val loss 0.0862\n",
      "step 2700: train loss 0.0731, val loss 0.0855\n",
      "step 2800: train loss 0.0727, val loss 0.0864\n",
      "step 2900: train loss 0.0750, val loss 0.0857\n",
      "step 3000: train loss 0.0730, val loss 0.0844\n",
      "step 3100: train loss 0.0702, val loss 0.0817\n",
      "step 3200: train loss 0.0719, val loss 0.0830\n",
      "step 3300: train loss 0.0733, val loss 0.0822\n",
      "step 3400: train loss 0.0689, val loss 0.0799\n",
      "step 3500: train loss 0.0705, val loss 0.0843\n",
      "step 3600: train loss 0.0705, val loss 0.0821\n",
      "step 3700: train loss 0.0712, val loss 0.0822\n",
      "step 3800: train loss 0.0697, val loss 0.0806\n",
      "step 3900: train loss 0.0710, val loss 0.0810\n",
      "step 4000: train loss 0.0693, val loss 0.0802\n",
      "step 4100: train loss 0.0686, val loss 0.0771\n",
      "step 4200: train loss 0.0674, val loss 0.0811\n",
      "step 4300: train loss 0.0686, val loss 0.0810\n",
      "step 4400: train loss 0.0673, val loss 0.0805\n",
      "step 4500: train loss 0.0682, val loss 0.0790\n",
      "step 4600: train loss 0.0669, val loss 0.0827\n",
      "step 4700: train loss 0.0671, val loss 0.0782\n",
      "step 4800: train loss 0.0668, val loss 0.0790\n",
      "step 4900: train loss 0.0677, val loss 0.0798\n",
      "step 4999: train loss 0.0659, val loss 0.0799\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(\n",
    "            f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "P\n",
      "PPPPPPPPPP%PP%%PPPPPPPPmPPPPPPPPPmPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPsPPPPPPPPPPPPP%PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP%PPPPPPPPPPPPPPsPPPPPPPPPPPPPPPPPsPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\n",
      "PPPPPPPPPPPPPPPPPPPP\n",
      "%PPPPPPPPPPPPPPPPPPPPPPPPP\n",
      "%%PPPPPFPPPb%PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPbPPP%PPPPPPPPPPPPPPPPPPPPPPPPPPPPPP40mPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\n",
      "PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPePPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP%PPPPPPPPPPPPPPPPPPPPPPPP%PPPPPP%PPPPPPP%PPPPPPPPPP%P%PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPgPPPaPPPPPPPPPPPPPPPPPPPPPPP%PPPPPPPPPPPPP%PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP%PPPPP%PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPaPPPPPPPPP\n",
      "PPP\"PPPPPPPPPP\"PPP\n",
      "%PP\n",
      "%\"PPPPPPPPPPPPPPPPPPPPP1%PPP0PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP7P–PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPsPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPAPPPPsPaPPPPPPPPPPP\n",
      "%PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\"PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPaPPPPPPPPP%PPPPPPPPP\n",
      "PPPPmPPPPPAbNPP0A1PPPPPPPPPPPPPPPPPPPPPPPPsPPPPPPPPPPPPPPPPPPPPPPP\n",
      "\"PamPPPPPPPPPPsPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP0PPPPPPPPPPPPPPPPPPPPPPPPPPPPsPPPPPPPP-PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\n",
      "% PPPPP6PPP\"PPPPPP\"PPP\"\"PPPPPPPPPPPPaPPPPPPPPPP\"PPPPPPPPPPPPPPPPPPP%PPPPPPPPPPPPPPP1\n",
      "PPPPPPPbPPPPPPspPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP%PPPPPPPPPPPPPPPPPPPPPPPPPPPaPPPPPPPPPPPPPPPPPPPPPPPPaP%PPPPPPPPPPPPPPPPPPPPPPPPPPPP%PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\"PPPPPPPPP\n",
      "PPPPPPPPPPPPPsPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPsAPPPPPFPPPPPPPPPPP7PPPPPPPPPPPPPPPPPPPPPPPPPPPPP w-SPPPPPPPPPPPPPPPPsPwPPPPPPPPsPPPPPPPPPPsPPPPnPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPmPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 57,\n",
       " 65,\n",
       " 72,\n",
       " 55,\n",
       " 68,\n",
       " 64,\n",
       " 65,\n",
       " 68,\n",
       " 81,\n",
       " 69,\n",
       " 1,\n",
       " 68,\n",
       " 51,\n",
       " 53,\n",
       " 55,\n",
       " 1,\n",
       " 59,\n",
       " 64,\n",
       " 1,\n",
       " 38,\n",
       " 65,\n",
       " 68,\n",
       " 70,\n",
       " 58,\n",
       " 1,\n",
       " 27,\n",
       " 51,\n",
       " 68,\n",
       " 65,\n",
       " 62,\n",
       " 59]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " governor’s race in North Caroli governor’s race in North Carolin\n",
      " a missive on Truth Social sayin a missive on Truth Social saying\n",
      "result in better political suppo esult in better political suppor\n",
      " a softer approach with business a softer approach with businesse\n",
      " and recognizing Israel’s annexa and recognizing Israel’s annexat\n",
      "er generations or other communit r generations or other communiti\n",
      "ban,” Trump said Thursday evenin an,” Trump said Thursday evening\n",
      "vAI, demonstrated to AFP the eas AI, demonstrated to AFP the ease\n",
      "icised Harris over the Biden adm cised Harris over the Biden admi\n",
      "ans lack of support from Jewish  ns lack of support from Jewish v\n",
      "r\" of visual disinformation, Han \" of visual disinformation, Hans\n",
      "on’s influence on gun laws. Miri n’s influence on gun laws. Miria\n",
      "he also referred to himself as a e also referred to himself as a \n",
      "t in place a version of his trav  in place a version of his trave\n",
      " US households, raising prices o US households, raising prices on\n",
      "ban but changes to the policy ev an but changes to the policy eve\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(xb)):\n",
    "    print(decode(xb[i].tolist()), decode(yb[i].tolist()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
